# -*- coding: utf-8 -*-
"""ArtisticStyleAnalysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L4SwjiUTgtTzOS1Lp6UwQ65x7vNDBj1X
"""

!pip install -U transformers huggingface_hub fiftyone umap-learn

pip install hf-transfer

import os
os.environ["HF_HUB_ENABLE_HF_TRANSFER"] = "1"

import fiftyone as fo  # base library and app
import fiftyone.zoo as foz  # zoo datasets and models
import fiftyone.brain as fob  # ML routines
from fiftyone import ViewField as F  # for defining custom views
import fiftyone.utils.huggingface as fouh  # for loading datasets from Hugging Face

dataset = fouh.load_from_hub(
    "huggan/wikiart",  ## repo_id
    format="parquet",  ## for Parquet format
    classification_fields=["artist", "style", "genre"],  # columns to store as classification fields
    max_samples=1000,  # number of samples to load
    name="wikiart",  # name of the dataset in FiftyOne
)

print(dataset)

# Launch the FiftyOne app
session = fo.launch_app(dataset)

# Add a small delay to allow the app to start
time.sleep(5) # You might need to adjust this delay

# Explicitly connect the client if needed (though launch_app usually handles this)
# In some edge cases, forcing a reconnect might help
try:
    session.connect()
except Exception as e:
    print(f"Could not explicitly connect to session: {e}")

artists = dataset.distinct("artist.label")
print(artists)

fob.compute_similarity(
    dataset,
    model="zero-shot-classification-transformer-torch",  ## type of model to load from model zoo
    name_or_path="openai/clip-vit-base-patch32",  ## repo_id of checkpoint
    embeddings="clip_embeddings",  ## name of the field to store embeddings
    brain_key="clip_sim",  ## key to store similarity index info
    batch_size=32,  ## batch size for inference
)

fob.compute_visualization(dataset, embeddings="clip_embeddings", method="umap", brain_key="clip_vis")

!fiftyone plugins download https://github.com/jacobmarks/clustering-plugin

fob.compute_uniqueness(dataset, embeddings="clip_embeddings")  # compute uniqueness using CLIP embeddings

most_unique_view = dataset.sort_by("uniqueness", reverse=True)
session.view = most_unique_view.view()  # Most unique images

least_unique_view = dataset.sort_by("uniqueness", reverse=False)
session.view = least_unique_view.view()  # Least unique images

artist_unique_scores = {
    artist: dataset.match(F("artist.label") == artist).mean("uniqueness") for artist in artists
}

sorted_artists = sorted(artist_unique_scores, key=artist_unique_scores.get, reverse=True)

for artist in sorted_artists:
    print(f"{artist}: {artist_unique_scores[artist]}")

kustodiev_view = dataset.match(F("artist.label") == "boris-kustodiev")
session.view = kustodiev_view.view()

!fiftyone plugins download https://github.com/jacobmarks/image-quality-issues/